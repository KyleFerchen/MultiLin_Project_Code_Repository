{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyBigWig\n",
    "import re\n",
    "import h5py\n",
    "import multiprocess as mp\n",
    "from functools import partial\n",
    "\n",
    "os.chdir(\"/media/kyle_storage/kyle_ferchen/grimes_lab_main/analysis/\"\\\n",
    "    \"2023_06_12_tea_seq_atac_processing/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a path for the genome fasta file\n",
    "path_to_mm10 = \"/media/kyle_storage/kyle_ferchen/grimes_lab_main/reference/genomes/mm10/mm10.fa\"\n",
    "# Read in the mm10 fasta indexing file\n",
    "fai_mm10 = pd.read_table(path_to_mm10 + \".fai\", header=None)\n",
    "fai_mm10.columns = [\"NAME\", \"LENGTH\", \"OFFSET\", \"LINEBASES\", \"LINEWIDTH\"]\n",
    "fai_mm10 = fai_mm10.set_index(\"NAME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Helper functions\n",
    "\n",
    "# Define helper function to extract sequences from fasta file\n",
    "def read_seq_from_fasta(input_bed, fai_annotation, fasta):\n",
    "    bed_df = input_bed.copy()\n",
    "    # Correct start values for 0 index\n",
    "    bed_df.iloc[:,1] = bed_df.iloc[:,1].values - 1\n",
    "    # Define fai annotation based parameters for reading from fasta file\n",
    "    bed_df[\"offset\"] = [fai_annotation.loc[i, \"OFFSET\"] for i in \\\n",
    "        bed_df.iloc[:,0].values]\n",
    "    bed_df[\"lw\"] = [fai_annotation.loc[i, \"LINEWIDTH\"] for i in \\\n",
    "        bed_df.iloc[:,0].values]\n",
    "    bed_df[\"lb\"] = [fai_annotation.loc[i, \"LINEBASES\"] for i in \\\n",
    "        bed_df.iloc[:,0].values]\n",
    "    # Define positions in file to which to seek\n",
    "    bed_df[\"len_new_line\"] = bed_df[\"lw\"] - bed_df[\"lb\"]\n",
    "    bed_df[\"lines_to_start\"] = bed_df.iloc[:,1].values // bed_df[\"lb\"].values\n",
    "    bed_df[\"char_to_start\"] = bed_df[\"lines_to_start\"] * bed_df[\"lw\"]\n",
    "    bed_df[\"bases_before_start\"] = bed_df.iloc[:,1].values % bed_df[\"lb\"].values\n",
    "    bed_df[\"bases_to_read\"] = bed_df.iloc[:,2] - bed_df.iloc[:,1]\n",
    "    bed_df[\"bases_to_next_line\"] = bed_df[\"lb\"] - bed_df[\"bases_before_start\"]\n",
    "    # Check bases to next line\n",
    "    mask_need_next_line = (bed_df[\"bases_to_read\"] < \\\n",
    "        bed_df[\"bases_to_next_line\"]).values\n",
    "    # Define number of char values to read\n",
    "    bed_df[\"char_to_read\"] = 0\n",
    "    if mask_need_next_line.sum() > 0:\n",
    "        bed_df.loc[mask_need_next_line, \"char_to_read\"] = bed_df.loc[\\\n",
    "            mask_need_next_line, \"bases_to_read\"]\n",
    "    if (~mask_need_next_line).sum() > 0:\n",
    "        new_lines_to_read = 1 + ((\\\n",
    "            bed_df.loc[~mask_need_next_line, \"bases_to_read\"].values - \\\n",
    "            bed_df.loc[~mask_need_next_line, \"bases_to_next_line\"].values) // \\\n",
    "                bed_df.loc[~mask_need_next_line, \"lb\"].values)\n",
    "        bed_df.loc[~mask_need_next_line, \"char_to_read\"] = (\\\n",
    "            bed_df.loc[~mask_need_next_line, \"len_new_line\"].values * \\\n",
    "                new_lines_to_read) + \\\n",
    "                    bed_df.loc[~mask_need_next_line, \"bases_to_read\"].values\n",
    "\n",
    "    # Filter df to only the columns needed (clear memory)\n",
    "    bed_df = bed_df[[\\\n",
    "        \"offset\", \"char_to_start\", \"bases_before_start\", \"char_to_read\"]]\n",
    "    # Read in the sequence from the fasta\n",
    "    output_seqs = []\n",
    "    with open(fasta, \"r\") as f:\n",
    "        for i, row in bed_df.iterrows():\n",
    "            f.seek(\\\n",
    "                row[\"offset\"] + \\\n",
    "                row[\"char_to_start\"] + \\\n",
    "                row[\"bases_before_start\"])\n",
    "            output_seqs.append(f.read(row[\"char_to_read\"]).replace(\"\\n\", \"\"))\n",
    "    \n",
    "    return(output_seqs)\n",
    "\n",
    "def get_reverse_complement(seq):\n",
    "    # Define replacements\n",
    "    dict_replace = {\\\n",
    "        \"A\": \"T\", \n",
    "        \"C\": \"G\",\n",
    "        \"G\": \"C\",\n",
    "        \"T\": \"A\",\n",
    "        \"a\": \"t\",\n",
    "        \"c\": \"g\",\n",
    "        \"g\": \"c\",\n",
    "        \"t\": \"a\",\n",
    "        \"n\": \"n\",\n",
    "        \"N\": \"N\"}\n",
    "    # Replace and return\n",
    "    return(\"\".join([dict_replace[i] for i in seq[::-1]]))\n",
    "\n",
    "\n",
    "def read_bw_instances_from_bed_df(bw_file_path, bed_df, cores=None):\n",
    "    if cores == None:\n",
    "        # create as many processes as there are CPUs on your machine\n",
    "        num_processes = mp.cpu_count()\n",
    "    else:\n",
    "        num_processes = cores\n",
    "    \n",
    "    # calculate the chunk size as an integer\n",
    "    chunk_size = bed_df.shape[0] // num_processes\n",
    "    # Assign chunks\n",
    "    chunk_starts = list(range(0, bed_df.shape[0], chunk_size))\n",
    "    chunks = [bed_df.iloc[tmp_start:chunk_starts[i+1]] for i,tmp_start in \\\n",
    "        enumerate(chunk_starts[:-2])]\n",
    "    if (bed_df.shape[0] % num_processes) == 0:\n",
    "        chunks += [\\\n",
    "            bed_df.iloc[chunk_starts[-2]:chunk_starts[-1]],\n",
    "            bed_df.iloc[chunk_starts[-1]:]]\n",
    "    else:\n",
    "        chunks += [bed_df.iloc[chunk_starts[-2]:]]\n",
    "\n",
    "    def _help_read_bigwig(input_bed_df):\n",
    "        with pyBigWig.open(bw_file_path) as tmp_bw:\n",
    "            bw_series = pd.Series([\\\n",
    "                np.array(tmp_bw.values(\\\n",
    "                    row[\"chr\"], \n",
    "                    row[\"start\"]-1,\n",
    "                    row[\"end\"])) for i, row in input_bed_df.iterrows()],\n",
    "                index=input_bed_df.index.values)\n",
    "            \n",
    "        return(bw_series)\n",
    "\n",
    "    # Create pool with `num_processes` processes\n",
    "    pool = mp.Pool(processes=num_processes)\n",
    "    # Apply function to each chunk\n",
    "    result = pool.map(_help_read_bigwig, chunks)\n",
    "    return(pd.concat(result).loc[bed_df.index.values])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chr</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>name</th>\n",
       "      <th>score</th>\n",
       "      <th>strand</th>\n",
       "      <th>thickStart</th>\n",
       "      <th>thickEnd</th>\n",
       "      <th>color</th>\n",
       "      <th>summit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>chr1:4456181-4457181</th>\n",
       "      <td>chr1</td>\n",
       "      <td>4456181</td>\n",
       "      <td>4457181</td>\n",
       "      <td>peak_1</td>\n",
       "      <td>1000</td>\n",
       "      <td>.</td>\n",
       "      <td>4456181</td>\n",
       "      <td>4457181</td>\n",
       "      <td>255,0,220</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chr1:4540111-4541111</th>\n",
       "      <td>chr1</td>\n",
       "      <td>4540111</td>\n",
       "      <td>4541111</td>\n",
       "      <td>peak_2</td>\n",
       "      <td>1000</td>\n",
       "      <td>.</td>\n",
       "      <td>4540111</td>\n",
       "      <td>4541111</td>\n",
       "      <td>255,0,220</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chr1:4614190-4615190</th>\n",
       "      <td>chr1</td>\n",
       "      <td>4614190</td>\n",
       "      <td>4615190</td>\n",
       "      <td>peak_3</td>\n",
       "      <td>1000</td>\n",
       "      <td>.</td>\n",
       "      <td>4614190</td>\n",
       "      <td>4615190</td>\n",
       "      <td>255,0,220</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chr1:4615468-4616468</th>\n",
       "      <td>chr1</td>\n",
       "      <td>4615468</td>\n",
       "      <td>4616468</td>\n",
       "      <td>peak_4</td>\n",
       "      <td>1000</td>\n",
       "      <td>.</td>\n",
       "      <td>4615468</td>\n",
       "      <td>4616468</td>\n",
       "      <td>255,0,220</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chr1:4621593-4622593</th>\n",
       "      <td>chr1</td>\n",
       "      <td>4621593</td>\n",
       "      <td>4622593</td>\n",
       "      <td>peak_5</td>\n",
       "      <td>1000</td>\n",
       "      <td>.</td>\n",
       "      <td>4621593</td>\n",
       "      <td>4622593</td>\n",
       "      <td>255,0,220</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chrX:169263066-169264066</th>\n",
       "      <td>chrX</td>\n",
       "      <td>169263066</td>\n",
       "      <td>169264066</td>\n",
       "      <td>peak_116374</td>\n",
       "      <td>1000</td>\n",
       "      <td>.</td>\n",
       "      <td>169263066</td>\n",
       "      <td>169264066</td>\n",
       "      <td>255,0,220</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chrX:169266871-169267871</th>\n",
       "      <td>chrX</td>\n",
       "      <td>169266871</td>\n",
       "      <td>169267871</td>\n",
       "      <td>peak_116375</td>\n",
       "      <td>1000</td>\n",
       "      <td>.</td>\n",
       "      <td>169266871</td>\n",
       "      <td>169267871</td>\n",
       "      <td>255,0,220</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chrX:169299283-169300283</th>\n",
       "      <td>chrX</td>\n",
       "      <td>169299283</td>\n",
       "      <td>169300283</td>\n",
       "      <td>peak_116376</td>\n",
       "      <td>1000</td>\n",
       "      <td>.</td>\n",
       "      <td>169299283</td>\n",
       "      <td>169300283</td>\n",
       "      <td>255,0,220</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chrX:169303654-169304654</th>\n",
       "      <td>chrX</td>\n",
       "      <td>169303654</td>\n",
       "      <td>169304654</td>\n",
       "      <td>peak_116377</td>\n",
       "      <td>1000</td>\n",
       "      <td>.</td>\n",
       "      <td>169303654</td>\n",
       "      <td>169304654</td>\n",
       "      <td>255,0,220</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chrX:169331986-169332986</th>\n",
       "      <td>chrX</td>\n",
       "      <td>169331986</td>\n",
       "      <td>169332986</td>\n",
       "      <td>peak_116378</td>\n",
       "      <td>1000</td>\n",
       "      <td>.</td>\n",
       "      <td>169331986</td>\n",
       "      <td>169332986</td>\n",
       "      <td>255,0,220</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>116378 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           chr      start        end         name  score  \\\n",
       "chr1:4456181-4457181      chr1    4456181    4457181       peak_1   1000   \n",
       "chr1:4540111-4541111      chr1    4540111    4541111       peak_2   1000   \n",
       "chr1:4614190-4615190      chr1    4614190    4615190       peak_3   1000   \n",
       "chr1:4615468-4616468      chr1    4615468    4616468       peak_4   1000   \n",
       "chr1:4621593-4622593      chr1    4621593    4622593       peak_5   1000   \n",
       "...                        ...        ...        ...          ...    ...   \n",
       "chrX:169263066-169264066  chrX  169263066  169264066  peak_116374   1000   \n",
       "chrX:169266871-169267871  chrX  169266871  169267871  peak_116375   1000   \n",
       "chrX:169299283-169300283  chrX  169299283  169300283  peak_116376   1000   \n",
       "chrX:169303654-169304654  chrX  169303654  169304654  peak_116377   1000   \n",
       "chrX:169331986-169332986  chrX  169331986  169332986  peak_116378   1000   \n",
       "\n",
       "                         strand  thickStart   thickEnd      color  summit  \n",
       "chr1:4456181-4457181          .     4456181    4457181  255,0,220     500  \n",
       "chr1:4540111-4541111          .     4540111    4541111  255,0,220     500  \n",
       "chr1:4614190-4615190          .     4614190    4615190  255,0,220     500  \n",
       "chr1:4615468-4616468          .     4615468    4616468  255,0,220     500  \n",
       "chr1:4621593-4622593          .     4621593    4622593  255,0,220     500  \n",
       "...                         ...         ...        ...        ...     ...  \n",
       "chrX:169263066-169264066      .   169263066  169264066  255,0,220     500  \n",
       "chrX:169266871-169267871      .   169266871  169267871  255,0,220     500  \n",
       "chrX:169299283-169300283      .   169299283  169300283  255,0,220     500  \n",
       "chrX:169303654-169304654      .   169303654  169304654  255,0,220     500  \n",
       "chrX:169331986-169332986      .   169331986  169332986  255,0,220     500  \n",
       "\n",
       "[116378 rows x 10 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the peak set used\n",
    "peaks = pd.read_table(\\\n",
    "    \"output/correlate_tea_atac_to_cite_rna_across_r7_clusters/\"\\\n",
    "    \"peak_to_gene_correlation_within_tads/sig_peak_to_gene_peaks_10col.bed\",\n",
    "    header=None)\n",
    "peaks.index = (\\\n",
    "    peaks.iloc[:,0] + \":\" + \\\n",
    "    peaks.iloc[:,1].astype(str) + \"-\" + \\\n",
    "    peaks.iloc[:,2].astype(str)).values\n",
    "peaks = peaks.loc[~pd.Series(peaks.index.values).duplicated().values]\n",
    "peaks.columns = [\"chr\", \"start\", \"end\", \"name\", \"score\", \"strand\", \"thickStart\",\n",
    "    \"thickEnd\", \"color\", \"summit\"]\n",
    "peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading positive patterns...\n",
      "Mask to positions in pattern above 0.5 frequency for a given base...\n"
     ]
    }
   ],
   "source": [
    "# Read in CWMs to do dot product scoring\n",
    "print(\"Reading positive patterns...\")\n",
    "path_to_h5_files = \"output/chrombpnet/modisco_merged_results/fold_0/\"\\\n",
    "    \"modisco_h5_outputs/\"\n",
    "h5_cluster_re = re.compile(r'modisco_fold_0_(.+)_modisco.h5')\n",
    "pat_dict = {}\n",
    "for tmp_h5_file in os.listdir(path_to_h5_files):\n",
    "    tmp_cluster = h5_cluster_re.findall(tmp_h5_file)[0]\n",
    "    tmp_h5_file_path = os.path.join(\\\n",
    "        path_to_h5_files, \n",
    "        tmp_h5_file)\n",
    "    with h5py.File(tmp_h5_file_path, \"r\") as tmp_h5:\n",
    "        if 'pos_patterns' in list(tmp_h5):\n",
    "            tmp_pos_pats = list(tmp_h5['pos_patterns'])\n",
    "            for tmp_pat in list(tmp_h5['pos_patterns']):\n",
    "                pat_dict[f\"{tmp_cluster}__pos_{tmp_pat}\"] = np.array(\\\n",
    "                    tmp_h5['pos_patterns'][tmp_pat]['sequence'][()]).astype(\\\n",
    "                        np.float32) \n",
    "                \n",
    "        if 'neg_patterns' in list(tmp_h5):\n",
    "            tmp_neg_pats = list(tmp_h5['neg_patterns'])\n",
    "            for tmp_pat in list(tmp_h5['neg_patterns']):\n",
    "                pat_dict[f\"{tmp_cluster}__neg_{tmp_pat}\"] = np.array(\\\n",
    "                    tmp_h5['neg_patterns'][tmp_pat]['sequence'][()]).astype(\\\n",
    "                        np.float32) \n",
    "\n",
    "# Modify pos_patterns -only use bases with frequencies above 0.5\n",
    "print(\"Mask to positions in pattern above 0.5 frequency for a given base...\")\n",
    "for tmp_pat in pat_dict:\n",
    "    pat_dict[tmp_pat] = (pat_dict[tmp_pat].T * \\\n",
    "        (pat_dict[tmp_pat] > 0.5).max(axis=1).astype(np.float32)).T\n",
    "    \n",
    "# Make reverse complement pat_dict\n",
    "pat_dict_rev_comp = {}\n",
    "for tmp_pat in pat_dict:\n",
    "    pat_dict_rev_comp[tmp_pat] = pat_dict[tmp_pat][::-1,::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on cluster BMCP...\n",
      "Working on cluster CD127_MP...\n",
      "Working on cluster CLP1_Rrm2...\n",
      "Working on cluster eHSC...\n",
      "Working on cluster eHSC_Pcna...\n",
      "Working on cluster ERP1...\n",
      "Working on cluster ERP2...\n",
      "Working on cluster HSCP_ERP1...\n",
      "Working on cluster HSCP_HPC_Cenpf...\n",
      "Working on cluster HSCP_HPC_Hist1h2af...\n",
      "Working on cluster HSCP_HPC_Tk1...\n",
      "Working on cluster HSCP_MKP...\n",
      "Working on cluster IG2_MP...\n",
      "Working on cluster IG2_proNeu1...\n",
      "Working on cluster LT_HSC_Mllt3...\n",
      "Working on cluster MDP_Cpa3...\n",
      "Working on cluster MDP_Irf8...\n",
      "Working on cluster MEP...\n",
      "Working on cluster MKP...\n",
      "Working on cluster ML_cell_cycle...\n",
      "Working on cluster MPP4_Hlf...\n",
      "Working on cluster MPP4_Nkx2_3...\n",
      "Working on cluster MPP5_Egr1...\n",
      "Working on cluster MPP5_Flt3...\n",
      "Working on cluster MultiLin_1...\n",
      "Working on cluster MultiLin_1_MEP...\n",
      "Working on cluster MultiLin_2_F13a1...\n",
      "Working on cluster MultiLin_2_Ms4a3...\n",
      "Working on cluster pre_MultiLin_1...\n",
      "Working on cluster pre_MultiLin_2...\n",
      "Working on cluster proNeu_1...\n",
      "Working on cluster ST_HSC...\n"
     ]
    }
   ],
   "source": [
    "# Read in the contribution scores for each peak for each cluster\n",
    "path_to_bigwigs = \"output/chrombpnet/browser_visualization/\"\\\n",
    "    \"count_scores_bw_extended_peak_set/fold_0/\"\n",
    "\n",
    "list_bw = os.listdir(path_to_bigwigs)\n",
    "\n",
    "cs_dict = {}\n",
    "# # Get the contribution score values\n",
    "for tmp_cluster_bw in list_bw:\n",
    "    tmp_cluster_name = tmp_cluster_bw[:-3]\n",
    "    print(f\"Working on cluster {tmp_cluster_name}...\")\n",
    "    tmp_path_bw = os.path.join(path_to_bigwigs, tmp_cluster_bw)\n",
    "    cs_dict[tmp_cluster_name] = read_bw_instances_from_bed_df(\\\n",
    "        tmp_path_bw, peaks[[\"chr\", \"start\", \"end\"]], cores=8)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading in observed bp sequences...\n"
     ]
    }
   ],
   "source": [
    "# Get the base pairs for each peak\n",
    "print(\"Reading in observed bp sequences...\")\n",
    "peak_seqs = pd.Series(read_seq_from_fasta(\\\n",
    "        input_bed = peaks[[\"chr\", \"start\", \"end\"]],\n",
    "        fai_annotation = fai_mm10,\n",
    "        fasta = path_to_mm10),\n",
    "    index=peaks.index.values).str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform sequences to one-hot-encoded DNA vectors\n",
    "# Get the one-hot-encoded DNA values fo the modisco seqs\n",
    "ohed_dict = {\\\n",
    "    \"A\": [1.0, 0.0, 0.0, 0.0],\n",
    "    \"C\": [0.0, 1.0, 0.0, 0.0],\n",
    "    \"G\": [0.0, 0.0, 1.0, 0.0],\n",
    "    \"T\": [0.0, 0.0, 0.0, 1.0],\n",
    "    \"N\": [0.0, 0.0, 0.0, 0.0]}\n",
    "peak_seqs_ohe = peak_seqs.apply(\\\n",
    "    lambda x: np.array([ohed_dict[i] for i in [*x]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on BMCP...\n",
      "Working on CD127_MP...\n",
      "Working on CLP1_Rrm2...\n",
      "Working on eHSC...\n",
      "Working on eHSC_Pcna...\n",
      "Working on ERP1...\n",
      "Working on ERP2...\n",
      "Working on HSCP_ERP1...\n",
      "Working on HSCP_HPC_Cenpf...\n",
      "Working on HSCP_HPC_Hist1h2af...\n",
      "Working on HSCP_HPC_Tk1...\n",
      "Working on HSCP_MKP...\n",
      "Working on IG2_MP...\n",
      "Working on IG2_proNeu1...\n",
      "Working on LT_HSC_Mllt3...\n",
      "Working on MDP_Cpa3...\n",
      "Working on MDP_Irf8...\n",
      "Working on MEP...\n",
      "Working on MKP...\n",
      "Working on ML_cell_cycle...\n",
      "Working on MPP4_Hlf...\n",
      "Working on MPP4_Nkx2_3...\n",
      "Working on MPP5_Egr1...\n",
      "Working on MPP5_Flt3...\n",
      "Working on MultiLin_1...\n",
      "Working on MultiLin_1_MEP...\n",
      "Working on MultiLin_2_F13a1...\n",
      "Working on MultiLin_2_Ms4a3...\n",
      "Working on pre_MultiLin_1...\n",
      "Working on pre_MultiLin_2...\n",
      "Working on proNeu_1...\n",
      "Working on ST_HSC...\n"
     ]
    }
   ],
   "source": [
    "path_save_motif_hits = \"output/chrombpnet/modisco_merged_results/fold_0/\"\\\n",
    "    \"redo_extract_seqlets/cluster_pwm_hits/\"\n",
    "\n",
    "list_motif_hit_files = os.listdir(path_save_motif_hits)\n",
    "\n",
    "\n",
    "saved_stats = []\n",
    "saved_hits = []\n",
    "for tmp_file in list_motif_hit_files:\n",
    "    tmp_cluster = tmp_file[:-4]\n",
    "    print(f\"Working on {tmp_cluster}...\")\n",
    "    tmp_hits = pd.read_feather(os.path.join(path_save_motif_hits, tmp_file))\n",
    "    # Add name for seqlet coordinates\n",
    "    tmp_hits[\"seq_name\"] = (\\\n",
    "        tmp_hits[\"chr\"] + \":\" + \\\n",
    "        tmp_hits[\"start\"].astype(str) + \"-\" + \\\n",
    "        tmp_hits[\"end\"].astype(str)).values\n",
    "    # Remove hits with duplicate positions and patterns\n",
    "    tmp_hits = tmp_hits.loc[\\\n",
    "        ~(tmp_hits[\"seq_name\"]+tmp_hits[\"pattern\"]).duplicated().values]\n",
    "    # Save the indices with unique seqlet positions\n",
    "    unique_idx = tmp_hits.loc[~tmp_hits[\"seq_name\"].duplicated()].index.values\n",
    "    # Save the unique indices with reverse strand patterns\n",
    "    unique_rev_idx = tmp_hits.loc[unique_idx].loc[\\\n",
    "        tmp_hits.loc[unique_idx, \"strand\"] == \"-\", \"seq_name\"].values\n",
    "    # Define helper dataframe\n",
    "    indexing_df = pd.DataFrame({\\\n",
    "            \"idx\": tmp_hits.loc[unique_idx, \"peak\"].values,\n",
    "            \"start_position\": tmp_hits.loc[unique_idx, \"pos\"].values,\n",
    "            \"end_position\": (tmp_hits.loc[unique_idx, \"end\"] - \\\n",
    "                tmp_hits.loc[unique_idx, \"start\"] + \\\n",
    "                tmp_hits.loc[unique_idx, \"pos\"]).values},\n",
    "        index=tmp_hits.loc[unique_idx, \"seq_name\"].values)\n",
    "    # Read in the contribution scores for the seqlet positions\n",
    "    seg_cs = pd.Series([i[j:k] for i,j,k in zip(\\\n",
    "            cs_dict[tmp_cluster].loc[indexing_df[\"idx\"].values].values,\n",
    "            indexing_df[\"start_position\"].values,\n",
    "            indexing_df[\"end_position\"].values)],\n",
    "        index=indexing_df.index.values)\n",
    "    # Get the one-hot encoded DNA sequences\n",
    "    seg_ohe = pd.Series([i[j:k] for i,j,k in zip(\\\n",
    "            peak_seqs_ohe.loc[indexing_df[\"idx\"].values].values,\n",
    "            indexing_df[\"start_position\"].values,\n",
    "            indexing_df[\"end_position\"].values)],\n",
    "        index=indexing_df.index.values)\n",
    "\n",
    "    # Weight the nucleotide frequency of the motif\n",
    "    tmp_fwd_idx = tmp_hits.loc[tmp_hits[\"strand\"] == \"+\"].index.values\n",
    "    tmp_rev_idx = tmp_hits.loc[tmp_hits[\"strand\"] == \"-\"].index.values\n",
    "    base_scores_fwd = pd.Series([(i*j).sum(axis=1) for i,j in zip(\\\n",
    "        seg_ohe.loc[tmp_hits.loc[tmp_fwd_idx,\"seq_name\"].values].values,\n",
    "        pd.Series(pat_dict)[tmp_hits.loc[tmp_fwd_idx,\"pattern\"].values].values)], \n",
    "        index=tmp_fwd_idx)\n",
    "    base_scores_rev = pd.Series([(i*j).sum(axis=1) for i,j in zip(\\\n",
    "        seg_ohe.loc[tmp_hits.loc[tmp_rev_idx,\"seq_name\"].values].values,\n",
    "        pd.Series(pat_dict_rev_comp)[tmp_hits.loc[tmp_rev_idx,\"pattern\"].values].values)], \n",
    "        index=tmp_rev_idx)\n",
    "    base_scores = pd.concat([base_scores_fwd, base_scores_rev]).loc[\\\n",
    "        tmp_hits.index.values]\n",
    "\n",
    "    # Calculate dot-product scores\n",
    "    tmp_dp_scores = pd.Series(\n",
    "        [(i*j).sum() for i,j in zip(\\\n",
    "            base_scores.values,\n",
    "            seg_cs.loc[tmp_hits.loc[\\\n",
    "                base_scores.index.values, \n",
    "                \"seq_name\"].values])],\n",
    "        index=base_scores.index.values)\n",
    "\n",
    "    # Invert scores with negative patterns\n",
    "    neg_pat_idx = base_scores.index.values[\\\n",
    "        tmp_hits.loc[base_scores.index.values, \"pattern\"].str.contains(\\\n",
    "            \"neg_pattern\")]\n",
    "\n",
    "    if len(neg_pat_idx) > 0:\n",
    "        tmp_dp_scores.loc[neg_pat_idx] = -1 * tmp_dp_scores.loc[neg_pat_idx]\n",
    "\n",
    "    # Get the statistics for each pattern from modisco hits\n",
    "    tmp_modisco_hits = tmp_hits.loc[tmp_hits[\"in_modisco\"]]\n",
    "    list_unique_pats = tmp_modisco_hits[\"pattern\"].unique()\n",
    "    tmp_min = []\n",
    "    tmp_max = []\n",
    "    tmp_pt5 = []\n",
    "    tmp_pt95 = []\n",
    "    for tmp_pat in list_unique_pats:\n",
    "        seg_scores = tmp_dp_scores.loc[tmp_modisco_hits.loc[\\\n",
    "            tmp_modisco_hits[\"pattern\"] == tmp_pat].index.values].values\n",
    "        tmp_min.append(seg_scores.min())\n",
    "        tmp_max.append(seg_scores.max())\n",
    "        tmp_pt5.append(np.percentile(seg_scores, 5))\n",
    "        tmp_pt95.append(np.percentile(seg_scores, 95))\n",
    "\n",
    "    tmp_stats = pd.DataFrame({\\\n",
    "            \"min\": tmp_min,\n",
    "            \"max\": tmp_max,\n",
    "            \"pctile_5\": tmp_pt5,\n",
    "            \"pctile_95\": tmp_pt95},\n",
    "        index=list_unique_pats)\n",
    "\n",
    "    # Filter \n",
    "    sel_tmp_hits = tmp_hits.loc[\\\n",
    "        tmp_dp_scores[tmp_dp_scores >= tmp_stats.loc[\\\n",
    "            tmp_hits.loc[\\\n",
    "                tmp_dp_scores.index.values, \n",
    "                \"pattern\"].values, \"min\"].values].index.values]\n",
    "    sel_tmp_hits[\"dp_score\"] = tmp_dp_scores.loc[\\\n",
    "        sel_tmp_hits.index.values].values\n",
    "\n",
    "    saved_stats.append(tmp_stats)\n",
    "    saved_hits.append(sel_tmp_hits)\n",
    "\n",
    "\n",
    "saved_stats = pd.concat(saved_stats)\n",
    "saved_hits = pd.concat(saved_hits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all instances\n",
    "saved_hits.index = list(range(saved_hits.shape[0]))\n",
    "saved_hits.to_feather(\"output/chrombpnet/modisco_merged_results/fold_0/\"\\\n",
    "    \"redo_extract_seqlets/all_seqlit_hits_above_modisco_min_anno.fea\")\n",
    "\n",
    "# Save modisco dpscore stats\n",
    "saved_stats.reset_index().to_feather(\"output/chrombpnet/\"\\\n",
    "    \"modisco_merged_results/fold_0/redo_extract_seqlets/\"\\\n",
    "    \"modisco_seqlet_hit_stats.fea\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chr</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>peak</th>\n",
       "      <th>score</th>\n",
       "      <th>pos</th>\n",
       "      <th>strand</th>\n",
       "      <th>pattern</th>\n",
       "      <th>in_modisco</th>\n",
       "      <th>seq_name</th>\n",
       "      <th>dp_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>chr1</td>\n",
       "      <td>4456740</td>\n",
       "      <td>4456770</td>\n",
       "      <td>chr1:4456181-4457181</td>\n",
       "      <td>5.125949</td>\n",
       "      <td>560</td>\n",
       "      <td>+</td>\n",
       "      <td>BMCP__pos_pattern_32</td>\n",
       "      <td>False</td>\n",
       "      <td>chr1:4456740-4456770</td>\n",
       "      <td>0.318700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>chr1</td>\n",
       "      <td>4456742</td>\n",
       "      <td>4456772</td>\n",
       "      <td>chr1:4456181-4457181</td>\n",
       "      <td>8.421132</td>\n",
       "      <td>562</td>\n",
       "      <td>-</td>\n",
       "      <td>BMCP__pos_pattern_3</td>\n",
       "      <td>True</td>\n",
       "      <td>chr1:4456742-4456772</td>\n",
       "      <td>0.309506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>chr1</td>\n",
       "      <td>4456791</td>\n",
       "      <td>4456821</td>\n",
       "      <td>chr1:4456181-4457181</td>\n",
       "      <td>5.021894</td>\n",
       "      <td>611</td>\n",
       "      <td>+</td>\n",
       "      <td>BMCP__pos_pattern_5</td>\n",
       "      <td>False</td>\n",
       "      <td>chr1:4456791-4456821</td>\n",
       "      <td>0.153212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>chr1</td>\n",
       "      <td>4614242</td>\n",
       "      <td>4614272</td>\n",
       "      <td>chr1:4614190-4615190</td>\n",
       "      <td>4.184268</td>\n",
       "      <td>53</td>\n",
       "      <td>+</td>\n",
       "      <td>BMCP__pos_pattern_19</td>\n",
       "      <td>False</td>\n",
       "      <td>chr1:4614242-4614272</td>\n",
       "      <td>0.096635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>chr1</td>\n",
       "      <td>4614242</td>\n",
       "      <td>4614272</td>\n",
       "      <td>chr1:4614190-4615190</td>\n",
       "      <td>1.546088</td>\n",
       "      <td>53</td>\n",
       "      <td>+</td>\n",
       "      <td>BMCP__pos_pattern_21</td>\n",
       "      <td>False</td>\n",
       "      <td>chr1:4614242-4614272</td>\n",
       "      <td>0.092790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16097284</th>\n",
       "      <td>chrX</td>\n",
       "      <td>169300178</td>\n",
       "      <td>169300208</td>\n",
       "      <td>chrX:169299283-169300283</td>\n",
       "      <td>8.963812</td>\n",
       "      <td>896</td>\n",
       "      <td>-</td>\n",
       "      <td>ST_HSC__pos_pattern_1</td>\n",
       "      <td>False</td>\n",
       "      <td>chrX:169300178-169300208</td>\n",
       "      <td>0.413159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16097285</th>\n",
       "      <td>chrX</td>\n",
       "      <td>169300188</td>\n",
       "      <td>169300218</td>\n",
       "      <td>chrX:169299283-169300283</td>\n",
       "      <td>5.419419</td>\n",
       "      <td>906</td>\n",
       "      <td>+</td>\n",
       "      <td>ST_HSC__pos_pattern_17</td>\n",
       "      <td>False</td>\n",
       "      <td>chrX:169300188-169300218</td>\n",
       "      <td>0.442239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16097286</th>\n",
       "      <td>chrX</td>\n",
       "      <td>169304118</td>\n",
       "      <td>169304148</td>\n",
       "      <td>chrX:169303654-169304654</td>\n",
       "      <td>5.680791</td>\n",
       "      <td>465</td>\n",
       "      <td>-</td>\n",
       "      <td>ST_HSC__pos_pattern_34</td>\n",
       "      <td>False</td>\n",
       "      <td>chrX:169304118-169304148</td>\n",
       "      <td>0.512000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16097287</th>\n",
       "      <td>chrX</td>\n",
       "      <td>169304138</td>\n",
       "      <td>169304168</td>\n",
       "      <td>chrX:169303654-169304654</td>\n",
       "      <td>12.651671</td>\n",
       "      <td>485</td>\n",
       "      <td>+</td>\n",
       "      <td>ST_HSC__pos_pattern_1</td>\n",
       "      <td>False</td>\n",
       "      <td>chrX:169304138-169304168</td>\n",
       "      <td>2.852918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16097288</th>\n",
       "      <td>chrX</td>\n",
       "      <td>169332444</td>\n",
       "      <td>169332474</td>\n",
       "      <td>chrX:169331986-169332986</td>\n",
       "      <td>10.628412</td>\n",
       "      <td>459</td>\n",
       "      <td>+</td>\n",
       "      <td>ST_HSC__pos_pattern_1</td>\n",
       "      <td>False</td>\n",
       "      <td>chrX:169332444-169332474</td>\n",
       "      <td>2.075172</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16097289 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           chr      start        end                      peak      score  \\\n",
       "0         chr1    4456740    4456770      chr1:4456181-4457181   5.125949   \n",
       "1         chr1    4456742    4456772      chr1:4456181-4457181   8.421132   \n",
       "2         chr1    4456791    4456821      chr1:4456181-4457181   5.021894   \n",
       "3         chr1    4614242    4614272      chr1:4614190-4615190   4.184268   \n",
       "4         chr1    4614242    4614272      chr1:4614190-4615190   1.546088   \n",
       "...        ...        ...        ...                       ...        ...   \n",
       "16097284  chrX  169300178  169300208  chrX:169299283-169300283   8.963812   \n",
       "16097285  chrX  169300188  169300218  chrX:169299283-169300283   5.419419   \n",
       "16097286  chrX  169304118  169304148  chrX:169303654-169304654   5.680791   \n",
       "16097287  chrX  169304138  169304168  chrX:169303654-169304654  12.651671   \n",
       "16097288  chrX  169332444  169332474  chrX:169331986-169332986  10.628412   \n",
       "\n",
       "          pos strand                 pattern  in_modisco  \\\n",
       "0         560      +    BMCP__pos_pattern_32       False   \n",
       "1         562      -     BMCP__pos_pattern_3        True   \n",
       "2         611      +     BMCP__pos_pattern_5       False   \n",
       "3          53      +    BMCP__pos_pattern_19       False   \n",
       "4          53      +    BMCP__pos_pattern_21       False   \n",
       "...       ...    ...                     ...         ...   \n",
       "16097284  896      -   ST_HSC__pos_pattern_1       False   \n",
       "16097285  906      +  ST_HSC__pos_pattern_17       False   \n",
       "16097286  465      -  ST_HSC__pos_pattern_34       False   \n",
       "16097287  485      +   ST_HSC__pos_pattern_1       False   \n",
       "16097288  459      +   ST_HSC__pos_pattern_1       False   \n",
       "\n",
       "                          seq_name  dp_score  \n",
       "0             chr1:4456740-4456770  0.318700  \n",
       "1             chr1:4456742-4456772  0.309506  \n",
       "2             chr1:4456791-4456821  0.153212  \n",
       "3             chr1:4614242-4614272  0.096635  \n",
       "4             chr1:4614242-4614272  0.092790  \n",
       "...                            ...       ...  \n",
       "16097284  chrX:169300178-169300208  0.413159  \n",
       "16097285  chrX:169300188-169300218  0.442239  \n",
       "16097286  chrX:169304118-169304148  0.512000  \n",
       "16097287  chrX:169304138-169304168  2.852918  \n",
       "16097288  chrX:169332444-169332474  2.075172  \n",
       "\n",
       "[16097289 rows x 11 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saved_hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on BMCP...\n",
      "Working on CD127_MP...\n",
      "Working on CLP1_Rrm2...\n",
      "Working on eHSC...\n",
      "Working on eHSC_Pcna...\n",
      "Working on ERP1...\n",
      "Working on ERP2...\n",
      "Working on HSCP_ERP1...\n",
      "Working on HSCP_HPC_Cenpf...\n",
      "Working on HSCP_HPC_Hist1h2af...\n",
      "Working on HSCP_HPC_Tk1...\n",
      "Working on HSCP_MKP...\n",
      "Working on IG2_MP...\n",
      "Working on IG2_proNeu1...\n",
      "Working on LT_HSC_Mllt3...\n",
      "Working on MDP_Cpa3...\n",
      "Working on MDP_Irf8...\n",
      "Working on MEP...\n",
      "Working on MKP...\n",
      "Working on ML_cell_cycle...\n",
      "Working on MPP4_Hlf...\n",
      "Working on MPP4_Nkx2_3...\n",
      "Working on MPP5_Egr1...\n",
      "Working on MPP5_Flt3...\n",
      "Working on MultiLin_1...\n",
      "Working on MultiLin_1_MEP...\n",
      "Working on MultiLin_2_F13a1...\n",
      "Working on MultiLin_2_Ms4a3...\n",
      "Working on pre_MultiLin_1...\n",
      "Working on pre_MultiLin_2...\n",
      "Working on proNeu_1...\n",
      "Working on ST_HSC...\n"
     ]
    }
   ],
   "source": [
    "### Extract dp-scores for all seqlets on all clusters\n",
    "# Save the indices with unique seqlet positions\n",
    "unique_idx = saved_hits.loc[~saved_hits[\"seq_name\"].duplicated()].index.values\n",
    "# Save the unique indices with reverse strand patterns\n",
    "unique_rev_idx = saved_hits.loc[unique_idx].loc[\\\n",
    "    saved_hits.loc[unique_idx, \"strand\"] == \"-\", \"seq_name\"].values\n",
    "\n",
    "dp_scores = {}\n",
    "for tmp_cluster in cs_dict:\n",
    "    print(f\"Working on {tmp_cluster}...\")\n",
    "    # Define helper dataframe\n",
    "    indexing_df = pd.DataFrame({\\\n",
    "            \"idx\": saved_hits.loc[unique_idx, \"peak\"].values,\n",
    "            \"start_position\": saved_hits.loc[unique_idx, \"pos\"].values,\n",
    "            \"end_position\": (saved_hits.loc[unique_idx, \"end\"] - \\\n",
    "                saved_hits.loc[unique_idx, \"start\"] + \\\n",
    "                saved_hits.loc[unique_idx, \"pos\"]).values},\n",
    "        index=saved_hits.loc[unique_idx, \"seq_name\"].values)\n",
    "    # Read in the contribution scores for the seqlet positions\n",
    "    seg_cs = pd.Series([i[j:k] for i,j,k in zip(\\\n",
    "            cs_dict[tmp_cluster].loc[indexing_df[\"idx\"].values].values,\n",
    "            indexing_df[\"start_position\"].values,\n",
    "            indexing_df[\"end_position\"].values)],\n",
    "        index=indexing_df.index.values)\n",
    "    # Get the one-hot encoded DNA sequences\n",
    "    seg_ohe = pd.Series([i[j:k] for i,j,k in zip(\\\n",
    "            peak_seqs_ohe.loc[indexing_df[\"idx\"].values].values,\n",
    "            indexing_df[\"start_position\"].values,\n",
    "            indexing_df[\"end_position\"].values)],\n",
    "        index=indexing_df.index.values)\n",
    "\n",
    "    # Weight the nucleotide frequency of the motif\n",
    "    tmp_fwd_idx = saved_hits.loc[saved_hits[\"strand\"] == \"+\"].index.values\n",
    "    tmp_rev_idx = saved_hits.loc[saved_hits[\"strand\"] == \"-\"].index.values\n",
    "    base_scores_fwd = pd.Series([(i*j).sum(axis=1) for i,j in zip(\\\n",
    "        seg_ohe.loc[saved_hits.loc[tmp_fwd_idx,\"seq_name\"].values].values,\n",
    "        pd.Series(pat_dict)[saved_hits.loc[\\\n",
    "            tmp_fwd_idx,\"pattern\"].values].values)], \n",
    "        index=tmp_fwd_idx)\n",
    "    base_scores_rev = pd.Series([(i*j).sum(axis=1) for i,j in zip(\\\n",
    "        seg_ohe.loc[saved_hits.loc[tmp_rev_idx,\"seq_name\"].values].values,\n",
    "        pd.Series(pat_dict_rev_comp)[saved_hits.loc[\\\n",
    "            tmp_rev_idx,\"pattern\"].values].values)], \n",
    "        index=tmp_rev_idx)\n",
    "    base_scores = pd.concat([base_scores_fwd, base_scores_rev]).loc[\\\n",
    "        saved_hits.index.values]\n",
    "\n",
    "    # Calculate dot-product scores\n",
    "    dp_scores[tmp_cluster] = pd.Series(\n",
    "        [(i*j).sum() for i,j in zip(\\\n",
    "            base_scores.values,\n",
    "            seg_cs.loc[saved_hits.loc[\\\n",
    "                base_scores.index.values, \n",
    "                \"seq_name\"].values])],\n",
    "        index=base_scores.index.values)\n",
    "    \n",
    "dp_scores = pd.DataFrame(dp_scores)\n",
    "dp_scores.reset_index().to_feather(\"output/chrombpnet/modisco_merged_results/\"\\\n",
    "    \"fold_0/redo_extract_seqlets/\"\\\n",
    "    \"all_seqlit_hits_above_modisco_min_dp_scores.fea\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyInfinityFlow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
